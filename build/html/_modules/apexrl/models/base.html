<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>apexrl.models.base &#8212; ApexRL 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../../../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=27fed22d" />
    <script src="../../../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../../../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../../../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for apexrl.models.base</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2026 GitHub@Apex_rl Developer</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="sd">&quot;&quot;&quot;Base classes for policy (actor) and value (critic) networks.</span>

<span class="sd">Users can implement custom Actor and Critic by inheriting from these</span>
<span class="sd">base classes and implementing the required abstract methods.</span>

<span class="sd">The framework supports:</span>
<span class="sd">- Continuous actions with Gaussian distribution</span>
<span class="sd">- Discrete actions (to be implemented)</span>
<span class="sd">- Flexible observation spaces (images, vectors, dicts)</span>
<span class="sd">- Flexible network architectures (MLP, CNN, RNN, etc.)</span>

<span class="sd">Example for continuous actions with custom CNN encoder:</span>
<span class="sd">    &gt;&gt;&gt; from apexrl.models.base import ContinuousActor, Critic</span>
<span class="sd">    &gt;&gt;&gt; import torch.nn as nn</span>
<span class="sd">    &gt;&gt;&gt;</span>
<span class="sd">    &gt;&gt;&gt; class CustomContinuousActor(ContinuousActor):</span>
<span class="sd">    ...     def __init__(self, obs_space, action_space, cfg):</span>
<span class="sd">    ...         super().__init__(obs_space, action_space, cfg)</span>
<span class="sd">    ...         # Custom network: CNN + MLP</span>
<span class="sd">    ...         self.encoder = nn.Sequential(</span>
<span class="sd">    ...             nn.Conv2d(3, 32, 3, stride=2), nn.ReLU(),</span>
<span class="sd">    ...             nn.Conv2d(32, 64, 3, stride=2), nn.ReLU(),</span>
<span class="sd">    ...             nn.Flatten(),</span>
<span class="sd">    ...         )</span>
<span class="sd">    ...         # Compute feature dim after conv</span>
<span class="sd">    ...         with torch.no_grad():</span>
<span class="sd">    ...             dummy = torch.zeros(1, *obs_space.shape)</span>
<span class="sd">    ...             feat_dim = self.encoder(dummy).shape[1]</span>
<span class="sd">    ...         self.head = nn.Sequential(</span>
<span class="sd">    ...             nn.Linear(feat_dim, 256), nn.ReLU(),</span>
<span class="sd">    ...             nn.Linear(256, self.action_dim),</span>
<span class="sd">    ...         )</span>
<span class="sd">    ...         self.log_std = nn.Parameter(torch.zeros(self.action_dim))</span>
<span class="sd">    ...</span>
<span class="sd">    ...     def forward(self, obs):</span>
<span class="sd">    ...         # Return action mean</span>
<span class="sd">    ...         features = self.encoder(obs)</span>
<span class="sd">    ...         return self.head(features)</span>
<span class="sd">    ...</span>
<span class="sd">    ...     def get_action_dist(self, obs):</span>
<span class="sd">    ...         mean = self.forward(obs)</span>
<span class="sd">    ...         std = torch.exp(self.log_std)</span>
<span class="sd">    ...         return torch.distributions.Normal(mean, std)</span>
<span class="sd">    ...</span>
<span class="sd">    ...     def act(self, obs, deterministic=False):</span>
<span class="sd">    ...         dist = self.get_action_dist(obs)</span>
<span class="sd">    ...         if deterministic:</span>
<span class="sd">    ...             return dist.mean, torch.zeros(...)</span>
<span class="sd">    ...         action = dist.sample()</span>
<span class="sd">    ...         return action, dist.log_prob(action).sum(-1)</span>
<span class="sd">    ...</span>
<span class="sd">    ...     def evaluate(self, obs, actions):</span>
<span class="sd">    ...         dist = self.get_action_dist(obs)</span>
<span class="sd">    ...         log_probs = dist.log_prob(actions).sum(-1)</span>
<span class="sd">    ...         entropy = dist.entropy().sum(-1)</span>
<span class="sd">    ...         return log_probs, entropy</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">abc</span><span class="w"> </span><span class="kn">import</span> <span class="n">ABC</span><span class="p">,</span> <span class="n">abstractmethod</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Union</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">gymnasium</span><span class="w"> </span><span class="kn">import</span> <span class="n">spaces</span>


<div class="viewcode-block" id="Actor">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.Actor">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Actor</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Base abstract class for all policy networks.</span>

<span class="sd">    This is the most general interface. For specific action types,</span>
<span class="sd">    use ContinuousActor or DiscreteActor.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs_space</span><span class="p">:</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Actor.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs_space: Observation space from gymnasium.</span>
<span class="sd">            action_space: Action space from gymnasium.</span>
<span class="sd">            cfg: Optional configuration dictionary for custom parameters.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obs_space</span> <span class="o">=</span> <span class="n">obs_space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_space</span> <span class="o">=</span> <span class="n">action_space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span> <span class="ow">or</span> <span class="p">{}</span>

        <span class="c1"># Store space information</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obs_shape</span> <span class="o">=</span> <span class="n">obs_space</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_shape</span> <span class="o">=</span> <span class="n">action_space</span><span class="o">.</span><span class="n">shape</span>

<div class="viewcode-block" id="Actor.forward">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.Actor.forward">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass to get action distribution parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations from environment. Can be:</span>
<span class="sd">                - torch.Tensor: Flat or multi-dimensional observations</span>
<span class="sd">                - Dict[str, torch.Tensor]: Dictionary of observations</span>

<span class="sd">        Returns:</span>
<span class="sd">            Action distribution parameters. Shape depends on implementation.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>


<div class="viewcode-block" id="Actor.act">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.Actor.act">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">act</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample actions from the policy.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations from environment.</span>
<span class="sd">            deterministic: If True, return deterministic action.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of:</span>
<span class="sd">                - actions: Sampled actions.</span>
<span class="sd">                - log_probs: Log probabilities of actions.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>


<div class="viewcode-block" id="Actor.evaluate">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.Actor.evaluate">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate actions for computing loss.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations from environment.</span>
<span class="sd">            actions: Actions to evaluate.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of:</span>
<span class="sd">                - log_probs: Log probabilities of actions.</span>
<span class="sd">                - entropy: Entropy of action distribution.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>
</div>



<div class="viewcode-block" id="ContinuousActor">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.ContinuousActor">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">ContinuousActor</span><span class="p">(</span><span class="n">Actor</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract base class for continuous action policies.</span>

<span class="sd">    Uses Gaussian (Normal) distribution for action sampling.</span>

<span class="sd">    Users must implement:</span>
<span class="sd">        - forward(): Returns action mean</span>
<span class="sd">        - get_action_dist(): Returns torch.distributions.Normal</span>
<span class="sd">        - act(): Sample actions (or use default implementation)</span>
<span class="sd">        - evaluate(): Evaluate actions (or use default implementation)</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; class MyActor(ContinuousActor):</span>
<span class="sd">        ...     def __init__(self, obs_space, action_space, cfg):</span>
<span class="sd">        ...         super().__init__(obs_space, action_space, cfg)</span>
<span class="sd">        ...         # Your custom network here</span>
<span class="sd">        ...         self.net = nn.Sequential(...)</span>
<span class="sd">        ...         self.log_std = nn.Parameter(torch.zeros(self.action_dim))</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def forward(self, obs):</span>
<span class="sd">        ...         return self.net(obs)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def get_action_dist(self, obs):</span>
<span class="sd">        ...         mean = self.forward(obs)</span>
<span class="sd">        ...         std = torch.exp(self.log_std)</span>
<span class="sd">        ...         return torch.distributions.Normal(mean, std)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs_space</span><span class="p">:</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Continuous Actor.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs_space: Observation space.</span>
<span class="sd">            action_space: Continuous action space (Box).</span>
<span class="sd">            cfg: Optional configuration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">action_space</span><span class="p">,</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Box</span><span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;ContinuousActor requires Box action space, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">action_space</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">obs_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">cfg</span><span class="p">)</span>

        <span class="c1"># Continuous action info</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">action_dim</span> <span class="o">=</span> <span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">action_space</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="mi">1</span>

        <span class="c1"># Tanh squashing flag (default True for bounded actions)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_tanh_squash</span> <span class="o">=</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;use_tanh_squash&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span> <span class="k">if</span> <span class="n">cfg</span> <span class="k">else</span> <span class="kc">True</span>

<div class="viewcode-block" id="ContinuousActor.get_action_dist">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.ContinuousActor.get_action_dist">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_action_dist</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Normal</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Gaussian action distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.distributions.Normal distribution.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>


<div class="viewcode-block" id="ContinuousActor.act">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.ContinuousActor.act">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">act</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample continuous actions from Gaussian distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations.</span>
<span class="sd">            deterministic: If True, return mean without sampling.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of (actions, log_probs).</span>
<span class="sd">            Actions are squashed with tanh if use_tanh_squash is True.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_action_dist</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">deterministic</span><span class="p">:</span>
            <span class="n">raw_action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">mean</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">raw_action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">rsample</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_tanh_squash</span><span class="p">:</span>
            <span class="c1"># Tanh squashing for bounded actions</span>
            <span class="c1"># experimental for SAC(Soft Actor-Critic)</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">raw_action</span><span class="p">)</span>
            <span class="c1"># Correction for tanh squashing in log_prob</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">raw_action</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">log_prob</span> <span class="o">-=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">action</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">raw_action</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">raw_action</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">log_prob</span></div>


<div class="viewcode-block" id="ContinuousActor.evaluate">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.ContinuousActor.evaluate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate continuous actions.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations.</span>
<span class="sd">            actions: Actions (potentially squashed with tanh).</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of (log_probs, entropy).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_action_dist</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_tanh_squash</span><span class="p">:</span>
            <span class="c1"># Inverse tanh to get raw actions</span>
            <span class="c1"># Clamp to avoid numerical issues</span>
            <span class="n">clamped_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">actions</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.999</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">)</span>
            <span class="n">raw_actions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">atanh</span><span class="p">(</span><span class="n">clamped_actions</span><span class="p">)</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">raw_actions</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">log_prob</span> <span class="o">-=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">actions</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">log_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">entropy</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">entropy</span></div>


<div class="viewcode-block" id="ContinuousActor.to">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.ContinuousActor.to">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">to</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;ContinuousActor&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Move to device and update action bounds.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span></div>
</div>



<div class="viewcode-block" id="DiscreteActor">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.DiscreteActor">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">DiscreteActor</span><span class="p">(</span><span class="n">Actor</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract base class for discrete action policies.</span>

<span class="sd">    Uses Categorical distribution for action sampling.</span>

<span class="sd">    Users must implement:</span>
<span class="sd">        - forward(): Returns action logits</span>
<span class="sd">        - get_action_dist(): Returns torch.distributions.Categorical</span>

<span class="sd">    Note: This is a placeholder for future implementation.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs_space</span><span class="p">:</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Discrete Actor.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs_space: Observation space.</span>
<span class="sd">            action_space: Discrete action space.</span>
<span class="sd">            cfg: Optional configuration.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">action_space</span><span class="p">,</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Discrete</span><span class="p">),</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;DiscreteActor requires Discrete action space, got </span><span class="si">{</span><span class="nb">type</span><span class="p">(</span><span class="n">action_space</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">obs_space</span><span class="p">,</span> <span class="n">action_space</span><span class="p">,</span> <span class="n">cfg</span><span class="p">)</span>

        <span class="c1"># Discrete action info</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_actions</span> <span class="o">=</span> <span class="n">action_space</span><span class="o">.</span><span class="n">n</span>

<div class="viewcode-block" id="DiscreteActor.get_action_dist">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.DiscreteActor.get_action_dist">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_action_dist</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">Categorical</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get Categorical action distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            torch.distributions.Categorical distribution.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>


<div class="viewcode-block" id="DiscreteActor.act">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.DiscreteActor.act">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">act</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">deterministic</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Sample discrete actions from Categorical distribution.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations.</span>
<span class="sd">            deterministic: If True, return argmax action.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of (actions, log_probs).</span>
<span class="sd">            Actions are integer indices.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_action_dist</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">deterministic</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">probs</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>

        <span class="n">log_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">action</span><span class="p">,</span> <span class="n">log_prob</span></div>


<div class="viewcode-block" id="DiscreteActor.evaluate">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.DiscreteActor.evaluate">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">evaluate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
        <span class="n">actions</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate discrete actions.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations.</span>
<span class="sd">            actions: Action indices.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Tuple of (log_probs, entropy).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_action_dist</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="n">log_prob</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">actions</span><span class="p">)</span>
        <span class="n">entropy</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">entropy</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">log_prob</span><span class="p">,</span> <span class="n">entropy</span></div>
</div>



<div class="viewcode-block" id="Critic">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.Critic">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">Critic</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">ABC</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Abstract base class for value networks.</span>

<span class="sd">    The critic estimates the value function V(s).</span>
<span class="sd">    Supports asymmetric actor-critic (different observations).</span>

<span class="sd">    Users must implement:</span>
<span class="sd">        - forward(): Returns value estimates</span>
<span class="sd">        - get_value(): Returns value estimates (can call forward)</span>

<span class="sd">    The network structure is completely flexible - can use any architecture</span>
<span class="sd">    that takes observations and outputs a scalar value.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; class MyCritic(Critic):</span>
<span class="sd">        ...     def __init__(self, obs_space, cfg):</span>
<span class="sd">        ...         super().__init__(obs_space, cfg)</span>
<span class="sd">        ...         # Your custom network</span>
<span class="sd">        ...         self.encoder = nn.Sequential(...)</span>
<span class="sd">        ...         self.value_head = nn.Linear(feature_dim, 1)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def forward(self, obs):</span>
<span class="sd">        ...         features = self.encoder(obs)</span>
<span class="sd">        ...         return self.value_head(features).squeeze(-1)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     def get_value(self, obs):</span>
<span class="sd">        ...         return self.forward(obs)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">obs_space</span><span class="p">:</span> <span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize Critic.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs_space: Observation space from gymnasium.</span>
<span class="sd">            cfg: Optional configuration dictionary.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obs_space</span> <span class="o">=</span> <span class="n">obs_space</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span> <span class="ow">or</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">obs_shape</span> <span class="o">=</span> <span class="n">obs_space</span><span class="o">.</span><span class="n">shape</span>

<div class="viewcode-block" id="Critic.forward">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.Critic.forward">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Forward pass to get value estimates.</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations. Can be:</span>
<span class="sd">                - torch.Tensor: Observations (any shape)</span>
<span class="sd">                - Dict[str, torch.Tensor]: Dictionary of observations</span>

<span class="sd">        Returns:</span>
<span class="sd">            Value estimates. Shape: (batch_size,)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>


<div class="viewcode-block" id="Critic.get_value">
<a class="viewcode-back" href="../../../API/apexrl.models.base.html#apexrl.models.Critic.get_value">[docs]</a>
    <span class="nd">@abstractmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">get_value</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">obs</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get value estimates.</span>

<span class="sd">        This is typically the same as forward(), but allows for special</span>
<span class="sd">        handling if needed (e.g., target networks).</span>

<span class="sd">        Args:</span>
<span class="sd">            obs: Observations.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Value estimates. Shape: (batch_size,)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span></div>
</div>

</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">ApexRL</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  <li><a href="../../apexrl.html">apexrl</a><ul>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2026, Atticlmr.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 9.0.4</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
    </div>

    

    
  </body>
</html>