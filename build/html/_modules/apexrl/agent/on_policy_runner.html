<!DOCTYPE html>

<html lang="en" data-content_root="../../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>apexrl.agent.on_policy_runner &#8212; ApexRL 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css?v=5ecbeea2" />
    <link rel="stylesheet" type="text/css" href="../../../_static/basic.css?v=b08954a9" />
    <link rel="stylesheet" type="text/css" href="../../../_static/alabaster.css?v=27fed22d" />
    <script src="../../../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../../../_static/doctools.js?v=fd6eb6e6"></script>
    <script src="../../../_static/sphinx_highlight.js?v=6ffebe34"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
   
  <link rel="stylesheet" href="../../../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <h1>Source code for apexrl.agent.on_policy_runner</h1><div class="highlight"><pre>
<span></span><span class="c1"># Copyright (c) 2026 GitHub@Apex_rl Developer</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="sd">&quot;&quot;&quot;On-policy runner for PPO and similar algorithms.</span>

<span class="sd">This module provides a runner class that handles the training loop,</span>
<span class="sd">logging, checkpointing, and environment interaction for on-policy RL algorithms.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">__future__</span><span class="w"> </span><span class="kn">import</span> <span class="n">annotations</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">collections</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">os</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">time</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">typing</span><span class="w"> </span><span class="kn">import</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Deque</span><span class="p">,</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">List</span><span class="p">,</span> <span class="n">Optional</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">,</span> <span class="n">Type</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">gymnasium</span><span class="w"> </span><span class="kn">import</span> <span class="n">spaces</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils.tensorboard</span><span class="w"> </span><span class="kn">import</span> <span class="n">SummaryWriter</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">apexrl.envs.vecenv</span><span class="w"> </span><span class="kn">import</span> <span class="n">VecEnv</span>


<div class="viewcode-block" id="OnPolicyRunner">
<a class="viewcode-back" href="../../../API/apexrl.agent.on_policy_runner.html#apexrl.agent.OnPolicyRunner">[docs]</a>
<span class="k">class</span><span class="w"> </span><span class="nc">OnPolicyRunner</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Runner for on-policy RL algorithms (primarily PPO).</span>

<span class="sd">    Handles the training loop, logging, checkpointing, and environment interaction.</span>

<span class="sd">    Features:</span>
<span class="sd">    - Default PPO algorithm, extensible to other on-policy algorithms</span>
<span class="sd">    - Automatic logging of reward components and environment metrics from extras</span>
<span class="sd">    - Flexible callback system for custom training logic</span>
<span class="sd">    - Built-in checkpointing and TensorBoard integration</span>

<span class="sd">    Usage - Minimal (Auto-create PPO agent):</span>
<span class="sd">        &gt;&gt;&gt; from apexrl.agent.on_policy_runner import OnPolicyRunner</span>
<span class="sd">        &gt;&gt;&gt; from apexrl.envs.vecenv import DummyVecEnv</span>
<span class="sd">        &gt;&gt;&gt; from apexrl.models.mlp import MLPActor, MLPCritic</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; env = DummyVecEnv(num_envs=4096, num_obs=48, num_actions=12)</span>
<span class="sd">        &gt;&gt;&gt; runner = OnPolicyRunner(</span>
<span class="sd">        ...     env=env,</span>
<span class="sd">        ...     algorithm=&quot;ppo&quot;,  # or just omit, defaults to &quot;ppo&quot;</span>
<span class="sd">        ...     actor_class=MLPActor,</span>
<span class="sd">        ...     critic_class=MLPCritic,</span>
<span class="sd">        ...     log_dir=&quot;./logs&quot;,</span>
<span class="sd">        ... )</span>
<span class="sd">        &gt;&gt;&gt; runner.learn(total_timesteps=10_000_000)</span>

<span class="sd">    Usage - With Pre-configured Agent:</span>
<span class="sd">        &gt;&gt;&gt; from apexrl.algorithms.ppo import PPO, PPOConfig</span>
<span class="sd">        &gt;&gt;&gt;</span>
<span class="sd">        &gt;&gt;&gt; cfg = PPOConfig(learning_rate=3e-4)</span>
<span class="sd">        &gt;&gt;&gt; agent = PPO(env=env, cfg=cfg, actor_class=MLPActor, critic_class=MLPCritic, ...)</span>
<span class="sd">        &gt;&gt;&gt; runner = OnPolicyRunner(agent=agent, env=env, cfg=cfg)</span>
<span class="sd">        &gt;&gt;&gt; runner.learn(total_timesteps=10_000_000)</span>

<span class="sd">    Environment Extras Format:</span>
<span class="sd">        The runner automatically extracts and logs metrics from environment extras.</span>
<span class="sd">        Environments should return extras in step() like this:</span>

<span class="sd">        &gt;&gt;&gt; extras = {</span>
<span class="sd">        ...     &quot;time_outs&quot;: time_outs,  # Required: bool tensor (num_envs,)</span>
<span class="sd">        ...</span>
<span class="sd">        ...     # Optional: Reward components (auto-accumulated per episode)</span>
<span class="sd">        ...     &quot;reward_components&quot;: {</span>
<span class="sd">        ...         &quot;velocity&quot;: velocity_reward,      # (num_envs,) - step-level reward</span>
<span class="sd">        ...         &quot;energy&quot;: -energy_penalty,        # (num_envs,)</span>
<span class="sd">        ...         &quot;stability&quot;: stability_reward,    # (num_envs,)</span>
<span class="sd">        ...     },</span>
<span class="sd">        ...</span>
<span class="sd">        ...     # Optional: Custom metrics (logged directly to tensorboard)</span>
<span class="sd">        ...     &quot;log&quot;: {</span>
<span class="sd">        ...         &quot;/reward/velocity_mean&quot;: velocity_reward.mean().item(),</span>
<span class="sd">        ...         &quot;/robot/height_mean&quot;: robot_height.mean().item(),</span>
<span class="sd">        ...         &quot;/episode/length_mean&quot;: episode_length.float().mean().item(),</span>
<span class="sd">        ...     },</span>
<span class="sd">        ... }</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Registry of supported algorithms</span>
    <span class="n">ALGORITHMS</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">env</span><span class="p">:</span> <span class="n">VecEnv</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Algorithm selection (mutually exclusive with agent)</span>
        <span class="n">algorithm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;ppo&quot;</span><span class="p">,</span>
        <span class="n">actor_class</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">critic_class</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">obs_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">actor_cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">critic_cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Or provide pre-created agent</span>
        <span class="n">agent</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Logging and saving</span>
        <span class="n">log_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">save_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Configuration</span>
        <span class="n">log_reward_components</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">log_interval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span>
        <span class="n">save_interval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Initialize the on-policy runner.</span>

<span class="sd">        Args:</span>
<span class="sd">            env: Vectorized environment.</span>
<span class="sd">            cfg: Algorithm configuration. If None and algorithm=&quot;ppo&quot;, uses PPOConfig().</span>

<span class="sd">            # Algorithm selection (when agent is not provided):</span>
<span class="sd">            algorithm: Algorithm name (&quot;ppo&quot;). Defaults to &quot;ppo&quot;.</span>
<span class="sd">            actor_class: Actor network class. Required if agent not provided.</span>
<span class="sd">            critic_class: Critic network class. Required if agent not provided.</span>
<span class="sd">            obs_space: Observation space. Required if agent not provided.</span>
<span class="sd">            action_space: Action space. Required if agent not provided.</span>
<span class="sd">            actor_cfg: Actor network configuration.</span>
<span class="sd">            critic_cfg: Critic network configuration.</span>

<span class="sd">            # Or provide pre-created agent:</span>
<span class="sd">            agent: Pre-configured algorithm instance (e.g., PPO).</span>

<span class="sd">            # Logging:</span>
<span class="sd">            log_dir: Directory for TensorBoard logs.</span>
<span class="sd">            save_dir: Directory for checkpoints. Defaults to log_dir.</span>
<span class="sd">            device: Device for training. Auto-detects if None.</span>
<span class="sd">            log_reward_components: Whether to log reward components from extras.</span>
<span class="sd">            log_interval: Logging interval in iterations.</span>
<span class="sd">            save_interval: Checkpoint saving interval.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If neither agent nor (actor_class, critic_class, spaces) provided.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">env</span> <span class="o">=</span> <span class="n">env</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span>
            <span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span>
        <span class="p">)</span>

        <span class="c1"># Create or use provided agent</span>
        <span class="k">if</span> <span class="n">agent</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span> <span class="o">=</span> <span class="n">agent</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="n">cfg</span> <span class="ow">or</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">agent</span><span class="p">,</span> <span class="s2">&quot;cfg&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Must provide cfg when agent doesn&#39;t have one&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Auto-create agent based on algorithm name</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_create_agent</span><span class="p">(</span>
                <span class="n">algorithm</span><span class="o">=</span><span class="n">algorithm</span><span class="p">,</span>
                <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span>
                <span class="n">cfg</span><span class="o">=</span><span class="n">cfg</span><span class="p">,</span>
                <span class="n">actor_class</span><span class="o">=</span><span class="n">actor_class</span><span class="p">,</span>
                <span class="n">critic_class</span><span class="o">=</span><span class="n">critic_class</span><span class="p">,</span>
                <span class="n">obs_space</span><span class="o">=</span><span class="n">obs_space</span><span class="p">,</span>
                <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
                <span class="n">actor_cfg</span><span class="o">=</span><span class="n">actor_cfg</span><span class="p">,</span>
                <span class="n">critic_cfg</span><span class="o">=</span><span class="n">critic_cfg</span><span class="p">,</span>
                <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="c1"># Logging setup</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_dir</span> <span class="o">=</span> <span class="n">log_dir</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span> <span class="o">=</span> <span class="n">save_dir</span> <span class="ow">or</span> <span class="n">log_dir</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_dir</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">writer</span> <span class="o">=</span> <span class="n">SummaryWriter</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">log_dir</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_dir</span> <span class="k">else</span> <span class="kc">None</span>

        <span class="c1"># Configuration</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">=</span> <span class="n">log_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_interval</span> <span class="o">=</span> <span class="n">save_interval</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_reward_components</span> <span class="o">=</span> <span class="n">log_reward_components</span>

        <span class="c1"># Training state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># Episode-level reward component tracking</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reward_components</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">current_reward_components</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Environment metrics log buffers (from extras[&quot;log&quot;])</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_buffers</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Deque</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">log_buffer_maxlen</span> <span class="o">=</span> <span class="mi">1000</span>

        <span class="c1"># Loss history</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Deque</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_history_maxlen</span> <span class="o">=</span> <span class="mi">1000</span>

        <span class="c1"># Callbacks</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Callable</span><span class="p">]]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;pre_iteration&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;post_iteration&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;pre_rollout&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;post_rollout&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;pre_update&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;post_update&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="p">}</span>

        <span class="c1"># Sync agent&#39;s writer with runner</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> <span class="s2">&quot;writer&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">writer</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">writer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">writer</span>

<div class="viewcode-block" id="OnPolicyRunner.register_algorithm">
<a class="viewcode-back" href="../../../API/apexrl.agent.on_policy_runner.html#apexrl.agent.OnPolicyRunner.register_algorithm">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">register_algorithm</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">agent_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">,</span> <span class="n">config_class</span><span class="p">:</span> <span class="n">Type</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Register a new algorithm for use with the runner.</span>

<span class="sd">        Args:</span>
<span class="sd">            name: Algorithm name (e.g., &quot;ppo&quot;, &quot;a2c&quot;).</span>
<span class="sd">            agent_class: The algorithm class (e.g., PPO).</span>
<span class="sd">            config_class: The configuration class (e.g., PPOConfig).</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">cls</span><span class="o">.</span><span class="n">ALGORITHMS</span><span class="p">[</span><span class="n">name</span><span class="o">.</span><span class="n">lower</span><span class="p">()]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;agent_class&quot;</span><span class="p">:</span> <span class="n">agent_class</span><span class="p">,</span>
            <span class="s2">&quot;config_class&quot;</span><span class="p">:</span> <span class="n">config_class</span><span class="p">,</span>
        <span class="p">}</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_create_agent</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">algorithm</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">env</span><span class="p">:</span> <span class="n">VecEnv</span><span class="p">,</span>
        <span class="n">cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">],</span>
        <span class="n">actor_class</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">],</span>
        <span class="n">critic_class</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Type</span><span class="p">],</span>
        <span class="n">obs_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">],</span>
        <span class="n">action_space</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">spaces</span><span class="o">.</span><span class="n">Space</span><span class="p">],</span>
        <span class="n">actor_cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
        <span class="n">critic_cfg</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">],</span>
        <span class="n">device</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Create algorithm agent based on name.&quot;&quot;&quot;</span>
        <span class="c1"># Lazy import and register PPO if not already registered</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">ALGORITHMS</span><span class="p">:</span>
            <span class="kn">from</span><span class="w"> </span><span class="nn">apexrl.algorithms.ppo</span><span class="w"> </span><span class="kn">import</span> <span class="n">PPO</span><span class="p">,</span> <span class="n">PPOConfig</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">register_algorithm</span><span class="p">(</span><span class="s2">&quot;ppo&quot;</span><span class="p">,</span> <span class="n">PPO</span><span class="p">,</span> <span class="n">PPOConfig</span><span class="p">)</span>

        <span class="n">algorithm</span> <span class="o">=</span> <span class="n">algorithm</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">algorithm</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ALGORITHMS</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unknown algorithm: </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2">. &quot;</span>
                <span class="sa">f</span><span class="s2">&quot;Registered: </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ALGORITHMS</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="n">algo_info</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ALGORITHMS</span><span class="p">[</span><span class="n">algorithm</span><span class="p">]</span>
        <span class="n">agent_class</span> <span class="o">=</span> <span class="n">algo_info</span><span class="p">[</span><span class="s2">&quot;agent_class&quot;</span><span class="p">]</span>
        <span class="n">config_class</span> <span class="o">=</span> <span class="n">algo_info</span><span class="p">[</span><span class="s2">&quot;config_class&quot;</span><span class="p">]</span>

        <span class="c1"># Use default config if not provided</span>
        <span class="k">if</span> <span class="n">cfg</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">cfg</span> <span class="o">=</span> <span class="n">config_class</span><span class="p">()</span>

        <span class="c1"># Validate required arguments</span>
        <span class="k">if</span> <span class="n">actor_class</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">critic_class</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;actor_class and critic_class are required when creating </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2"> agent&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">obs_space</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">action_space</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;obs_space and action_space are required when creating </span><span class="si">{</span><span class="n">algorithm</span><span class="si">}</span><span class="s2"> agent&quot;</span>
            <span class="p">)</span>

        <span class="c1"># Create agent</span>
        <span class="n">agent</span> <span class="o">=</span> <span class="n">agent_class</span><span class="p">(</span>
            <span class="n">env</span><span class="o">=</span><span class="n">env</span><span class="p">,</span>
            <span class="n">cfg</span><span class="o">=</span><span class="n">cfg</span><span class="p">,</span>
            <span class="n">actor_class</span><span class="o">=</span><span class="n">actor_class</span><span class="p">,</span>
            <span class="n">critic_class</span><span class="o">=</span><span class="n">critic_class</span><span class="p">,</span>
            <span class="n">obs_space</span><span class="o">=</span><span class="n">obs_space</span><span class="p">,</span>
            <span class="n">action_space</span><span class="o">=</span><span class="n">action_space</span><span class="p">,</span>
            <span class="n">actor_cfg</span><span class="o">=</span><span class="n">actor_cfg</span> <span class="ow">or</span> <span class="p">{},</span>
            <span class="n">critic_cfg</span><span class="o">=</span><span class="n">critic_cfg</span> <span class="ow">or</span> <span class="p">{},</span>
            <span class="n">log_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>  <span class="c1"># Runner handles logging</span>
            <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">agent</span><span class="p">,</span> <span class="n">cfg</span>

<div class="viewcode-block" id="OnPolicyRunner.add_callback">
<a class="viewcode-back" href="../../../API/apexrl.agent.on_policy_runner.html#apexrl.agent.OnPolicyRunner.add_callback">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">add_callback</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">event</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">callback</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Add a callback for a specific event.</span>

<span class="sd">        Args:</span>
<span class="sd">            event: Event name. One of:</span>
<span class="sd">                - &quot;pre_iteration&quot;: Called before each training iteration</span>
<span class="sd">                - &quot;post_iteration&quot;: Called after each training iteration</span>
<span class="sd">                - &quot;pre_rollout&quot;: Called before collecting rollout</span>
<span class="sd">                - &quot;post_rollout&quot;: Called after collecting rollout</span>
<span class="sd">                - &quot;pre_update&quot;: Called before policy update</span>
<span class="sd">                - &quot;post_update&quot;: Called after policy update</span>
<span class="sd">            callback: Callback function. Signature depends on event.</span>

<span class="sd">        Raises:</span>
<span class="sd">            ValueError: If event name is invalid.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">event</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unknown event: </span><span class="si">{</span><span class="n">event</span><span class="si">}</span><span class="s2">. Must be one of </span><span class="si">{</span><span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="p">[</span><span class="n">event</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_call_callbacks</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">event</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Call all callbacks for an event.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">callback</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="p">[]):</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">callback</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
            <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Warning: Callback failed for event &#39;</span><span class="si">{</span><span class="n">event</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<div class="viewcode-block" id="OnPolicyRunner.collect_rollout">
<a class="viewcode-back" href="../../../API/apexrl.agent.on_policy_runner.html#apexrl.agent.OnPolicyRunner.collect_rollout">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">collect_rollout</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Collect a rollout from the environment.</span>

<span class="sd">        Uses the agent&#39;s collect_rollout method with an extras callback</span>
<span class="sd">        to capture environment metrics.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_callbacks</span><span class="p">(</span><span class="s2">&quot;pre_rollout&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

        <span class="c1"># Collect rollout with extras callback for logging</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">collect_rollout</span><span class="p">(</span><span class="n">extras_callback</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_process_extras</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">total_timesteps</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_call_callbacks</span><span class="p">(</span><span class="s2">&quot;post_rollout&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">stats</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">stats</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_process_extras</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">extras</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
        <span class="n">dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">true_dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
        <span class="n">episode_rewards</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Process environment extras to extract and log metrics.</span>

<span class="sd">        This is called after each environment step during rollout collection.</span>
<span class="sd">        Extracts:</span>
<span class="sd">        1. Custom metrics from extras[&quot;log&quot;] -&gt; directly logged to tensorboard</span>
<span class="sd">        2. Reward components from extras[&quot;reward_components&quot;] -&gt; accumulated per episode</span>

<span class="sd">        Args:</span>
<span class="sd">            extras: Extras dict from env.step().</span>
<span class="sd">            dones: All done flags (including timeouts).</span>
<span class="sd">            true_dones: True done flags (excluding timeouts).</span>
<span class="sd">            episode_rewards: Current episode reward accumulator.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># === Process custom metrics from extras[&quot;log&quot;] ===</span>
        <span class="c1"># These are logged directly (not accumulated)</span>
        <span class="n">log_dict</span> <span class="o">=</span> <span class="n">extras</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;log&quot;</span><span class="p">,</span> <span class="p">{})</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">log_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Normalize key format (ensure starts with &quot;/&quot;)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">):</span>
                <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;/</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span>

            <span class="c1"># Convert tensor to float</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="k">if</span> <span class="n">value</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">else</span> <span class="n">value</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

            <span class="c1"># Store in buffer for batch logging</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_buffers</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">log_buffers</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">log_buffer_maxlen</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">log_buffers</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>

        <span class="c1"># === Process reward components from extras[&quot;reward_components&quot;] ===</span>
        <span class="c1"># These are accumulated per episode and logged at episode end</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_reward_components</span><span class="p">:</span>
            <span class="n">reward_components</span> <span class="o">=</span> <span class="n">extras</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;reward_components&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">reward_components</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_accumulate_reward_components</span><span class="p">(</span><span class="n">reward_components</span><span class="p">,</span> <span class="n">true_dones</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_accumulate_reward_components</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">components</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
        <span class="n">true_dones</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Accumulate reward components per episode.</span>

<span class="sd">        For each reward component:</span>
<span class="sd">        1. Accumulate step-level values during the episode</span>
<span class="sd">        2. When episode ends (true_dones), store the total</span>
<span class="sd">        3. Reset accumulator for completed episodes</span>

<span class="sd">        Args:</span>
<span class="sd">            components: Dict of {component_name: step_reward_tensor}.</span>
<span class="sd">            true_dones: Bool tensor indicating completed episodes.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="n">components</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="c1"># Ensure tensor</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="k">continue</span>
            <span class="n">values</span> <span class="o">=</span> <span class="n">values</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

            <span class="c1"># Initialize accumulator for this component if needed</span>
            <span class="n">key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;/reward_component/</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_reward_components</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">current_reward_components</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">num_envs</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span>
                <span class="p">)</span>

            <span class="c1"># Accumulate step rewards</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">current_reward_components</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">+=</span> <span class="n">values</span>

            <span class="c1"># Handle completed episodes</span>
            <span class="k">if</span> <span class="n">true_dones</span><span class="o">.</span><span class="n">any</span><span class="p">():</span>
                <span class="n">completed_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">true_dones</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">completed_values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_reward_components</span><span class="p">[</span><span class="n">key</span><span class="p">][</span>
                    <span class="n">completed_indices</span>
                <span class="p">]</span>

                <span class="c1"># Store completed episode totals</span>
                <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_components</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">reward_components</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">completed_values</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">():</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">reward_components</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">val</span><span class="p">))</span>

                <span class="c1"># Reset accumulators for completed episodes</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">current_reward_components</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">*=</span> <span class="p">(</span><span class="o">~</span><span class="n">true_dones</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

<div class="viewcode-block" id="OnPolicyRunner.update">
<a class="viewcode-back" href="../../../API/apexrl.agent.on_policy_runner.html#apexrl.agent.OnPolicyRunner.update">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update policy using collected rollout.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_callbacks</span><span class="p">(</span><span class="s2">&quot;pre_update&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>
        <span class="n">stats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_call_callbacks</span><span class="p">(</span><span class="s2">&quot;post_update&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">stats</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">stats</span></div>


<div class="viewcode-block" id="OnPolicyRunner.learn">
<a class="viewcode-back" href="../../../API/apexrl.agent.on_policy_runner.html#apexrl.agent.OnPolicyRunner.learn">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">learn</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">total_timesteps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_iterations</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Train the agent.</span>

<span class="sd">        Priority for determining training length:</span>
<span class="sd">        1. num_iterations argument (if provided)</span>
<span class="sd">        2. total_timesteps argument (if provided)</span>
<span class="sd">        3. cfg.max_iterations (if set in config)</span>
<span class="sd">        4. Raise error if none provided</span>

<span class="sd">        Args:</span>
<span class="sd">            total_timesteps: Total environment steps to train for.</span>
<span class="sd">            num_iterations: Number of policy iterations. Overrides total_timesteps.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Training history dict with keys: iterations, timesteps, episode_rewards, etc.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># Determine iterations (priority: num_iterations &gt; total_timesteps &gt; cfg.max_iterations)</span>
        <span class="n">transitions_per_iter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">num_steps</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">num_envs</span>

        <span class="k">if</span> <span class="n">num_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">total_iters</span> <span class="o">=</span> <span class="n">num_iterations</span>
        <span class="k">elif</span> <span class="n">total_timesteps</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">total_iters</span> <span class="o">=</span> <span class="n">total_timesteps</span> <span class="o">//</span> <span class="n">transitions_per_iter</span>
        <span class="k">elif</span> <span class="p">(</span>
            <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="p">,</span> <span class="s2">&quot;max_iterations&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">max_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="n">total_iters</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">max_iterations</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Must provide one of:</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;  - num_iterations argument to learn()</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;  - total_timesteps argument to learn()</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="s2">&quot;  - cfg.max_iterations in config (e.g., PPOConfig(max_iterations=100))&quot;</span>
            <span class="p">)</span>

        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Training for </span><span class="si">{</span><span class="n">total_iters</span><span class="si">}</span><span class="s2"> iterations (</span><span class="si">{</span><span class="n">total_iters</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">transitions_per_iter</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> steps)&quot;</span>
        <span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Batch size: </span><span class="si">{</span><span class="n">transitions_per_iter</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> transitions/iteration&quot;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
        <span class="n">last_log_time</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_time</span>

        <span class="c1"># Initialize history tracking</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;iterations&quot;</span><span class="p">:</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_history_maxlen</span><span class="p">),</span>
            <span class="s2">&quot;policy_loss&quot;</span><span class="p">:</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_history_maxlen</span><span class="p">),</span>
            <span class="s2">&quot;value_loss&quot;</span><span class="p">:</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_history_maxlen</span><span class="p">),</span>
            <span class="s2">&quot;entropy_loss&quot;</span><span class="p">:</span> <span class="n">collections</span><span class="o">.</span><span class="n">deque</span><span class="p">(</span><span class="n">maxlen</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">loss_history_maxlen</span><span class="p">),</span>
        <span class="p">}</span>

        <span class="n">history</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;iterations&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;timesteps&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;episode_rewards&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;episode_lengths&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;policy_losses&quot;</span><span class="p">:</span> <span class="p">[],</span>
            <span class="s2">&quot;value_losses&quot;</span><span class="p">:</span> <span class="p">[],</span>
        <span class="p">}</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">iteration</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">total_iters</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">=</span> <span class="n">iteration</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">iteration</span> <span class="o">=</span> <span class="n">iteration</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_call_callbacks</span><span class="p">(</span><span class="s2">&quot;pre_iteration&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span>

                <span class="c1"># Collect rollout and update</span>
                <span class="n">rollout_stats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">collect_rollout</span><span class="p">()</span>
                <span class="n">update_stats</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">()</span>

                <span class="c1"># Learning rate schedule</span>
                <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> <span class="s2">&quot;adjust_learning_rate&quot;</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">adjust_learning_rate</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">total_iters</span><span class="p">)</span>

                <span class="c1"># Record history</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">_update_history</span><span class="p">(</span><span class="n">iteration</span><span class="p">,</span> <span class="n">rollout_stats</span><span class="p">,</span> <span class="n">update_stats</span><span class="p">,</span> <span class="n">history</span><span class="p">)</span>

                <span class="c1"># Logging</span>
                <span class="k">if</span> <span class="n">iteration</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_log_iteration</span><span class="p">(</span>
                        <span class="n">iteration</span><span class="p">,</span>
                        <span class="n">total_iters</span><span class="p">,</span>
                        <span class="n">rollout_stats</span><span class="p">,</span>
                        <span class="n">update_stats</span><span class="p">,</span>
                        <span class="n">last_log_time</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="n">last_log_time</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

                <span class="c1"># Checkpointing</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span> <span class="ow">and</span> <span class="n">iteration</span> <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_interval</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;checkpoint_</span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">.pt&quot;</span><span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">_call_callbacks</span><span class="p">(</span>
                    <span class="s2">&quot;post_iteration&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="p">{</span><span class="o">**</span><span class="n">rollout_stats</span><span class="p">,</span> <span class="o">**</span><span class="n">update_stats</span><span class="p">}</span>
                <span class="p">)</span>

        <span class="k">except</span> <span class="ne">KeyboardInterrupt</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training interrupted by user&quot;</span><span class="p">)</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">save_checkpoint</span><span class="p">(</span><span class="s2">&quot;checkpoint_final.pt&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Training complete! Total steps: </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">{</span>
            <span class="s2">&quot;history&quot;</span><span class="p">:</span> <span class="n">history</span><span class="p">,</span>
            <span class="s2">&quot;final_iteration&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">,</span>
            <span class="s2">&quot;total_timesteps&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="p">,</span>
        <span class="p">}</span></div>


    <span class="k">def</span><span class="w"> </span><span class="nf">_update_history</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">iteration</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">rollout_stats</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
        <span class="n">update_stats</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
        <span class="n">history</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Update training history buffers.&quot;&quot;&quot;</span>
        <span class="n">history</span><span class="p">[</span><span class="s2">&quot;iterations&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s2">&quot;timesteps&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="p">)</span>
        <span class="n">history</span><span class="p">[</span><span class="s2">&quot;policy_losses&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">update_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;train/policy_loss&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="n">history</span><span class="p">[</span><span class="s2">&quot;value_losses&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">update_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;train/value_loss&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span><span class="p">[</span><span class="s2">&quot;iterations&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">iteration</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span><span class="p">[</span><span class="s2">&quot;policy_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">update_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;train/policy_loss&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span><span class="p">[</span><span class="s2">&quot;value_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">update_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;train/value_loss&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss_history</span><span class="p">[</span><span class="s2">&quot;entropy_loss&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">update_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;train/entropy_loss&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_rewards</span><span class="p">:</span>
            <span class="n">mean_reward</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_rewards</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_rewards</span>
            <span class="p">)</span>
            <span class="n">mean_length</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_lengths</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_lengths</span>
            <span class="p">)</span>
            <span class="n">history</span><span class="p">[</span><span class="s2">&quot;episode_rewards&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_reward</span><span class="p">)</span>
            <span class="n">history</span><span class="p">[</span><span class="s2">&quot;episode_lengths&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">mean_length</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_log_iteration</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">iteration</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">total_iters</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">rollout_stats</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
        <span class="n">update_stats</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">],</span>
        <span class="n">last_log_time</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log training progress for current iteration.&quot;&quot;&quot;</span>
        <span class="c1"># Calculate FPS</span>
        <span class="n">elapsed</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">last_log_time</span>
        <span class="n">fps</span> <span class="o">=</span> <span class="p">(</span>
            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">cfg</span><span class="o">.</span><span class="n">num_steps</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">num_envs</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_interval</span> <span class="o">/</span> <span class="n">elapsed</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">elapsed</span> <span class="o">&gt;</span> <span class="mi">0</span>
            <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>

        <span class="c1"># Console output</span>
        <span class="n">msg</span> <span class="o">=</span> <span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Iter </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">total_iters</span><span class="si">}</span><span class="s2"> | &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Steps </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="si">:</span><span class="s2">,</span><span class="si">}</span><span class="s2"> | &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;FPS </span><span class="si">{</span><span class="n">fps</span><span class="si">:</span><span class="s2">.0f</span><span class="si">}</span><span class="s2"> | &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Policy Loss </span><span class="si">{</span><span class="n">update_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;train/policy_loss&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;Value Loss </span><span class="si">{</span><span class="n">update_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;train/value_loss&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2"> | &quot;</span>
            <span class="sa">f</span><span class="s2">&quot;KL </span><span class="si">{</span><span class="n">update_stats</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;train/approx_kl&#39;</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">)</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_rewards</span><span class="p">:</span>
            <span class="n">mean_reward</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_rewards</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_rewards</span>
            <span class="p">)</span>
            <span class="n">msg</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot; | Reward </span><span class="si">{</span><span class="n">mean_reward</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

        <span class="c1"># TensorBoard logging</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="p">:</span>
            <span class="k">return</span>

        <span class="c1"># Time metrics</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;time/fps&quot;</span><span class="p">,</span> <span class="n">fps</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;time/iteration&quot;</span><span class="p">,</span> <span class="n">iteration</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="p">)</span>

        <span class="c1"># Rollout stats (vs timesteps)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">rollout_stats</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="p">)</span>

        <span class="c1"># Training stats (vs both timesteps and iteration)</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">update_stats</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span><span class="p">)</span>
            <span class="c1"># Also log with iteration for easier analysis</span>
            <span class="n">iter_key</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;train/&quot;</span><span class="p">,</span> <span class="s2">&quot;train_vs_iter/&quot;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">iter_key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">iteration</span><span class="p">)</span>

        <span class="c1"># Distribution stats</span>
        <span class="n">advantages</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">rollout_buffer</span><span class="o">.</span><span class="n">advantages</span>
        <span class="n">values</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">rollout_buffer</span><span class="o">.</span><span class="n">values</span>
        <span class="n">returns</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">rollout_buffer</span><span class="o">.</span><span class="n">returns</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span>
            <span class="s2">&quot;stats/advantage_mean&quot;</span><span class="p">,</span> <span class="n">advantages</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">iteration</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span>
            <span class="s2">&quot;stats/advantage_std&quot;</span><span class="p">,</span> <span class="n">advantages</span><span class="o">.</span><span class="n">std</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">iteration</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;stats/value_mean&quot;</span><span class="p">,</span> <span class="n">values</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">iteration</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="s2">&quot;stats/returns_mean&quot;</span><span class="p">,</span> <span class="n">returns</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="n">iteration</span><span class="p">)</span>

        <span class="c1"># Episode stats</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_rewards</span><span class="p">:</span>
            <span class="n">mean_reward</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_rewards</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_rewards</span>
            <span class="p">)</span>
            <span class="n">mean_length</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_lengths</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_lengths</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span>
                <span class="s2">&quot;episode/mean_reward&quot;</span><span class="p">,</span> <span class="n">mean_reward</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span>
                <span class="s2">&quot;episode/mean_length&quot;</span><span class="p">,</span> <span class="n">mean_length</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span>
                <span class="s2">&quot;episode_vs_iter/mean_reward&quot;</span><span class="p">,</span> <span class="n">mean_reward</span><span class="p">,</span> <span class="n">iteration</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span>
                <span class="s2">&quot;episode_vs_iter/mean_length&quot;</span><span class="p">,</span> <span class="n">mean_length</span><span class="p">,</span> <span class="n">iteration</span>
            <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_rewards</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">episode_lengths</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

        <span class="c1"># Log reward components</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_log_reward_components</span><span class="p">()</span>

        <span class="c1"># Log environment metrics from extras</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_log_environment_metrics</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_log_reward_components</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log accumulated reward components to TensorBoard.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">values</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">reward_components</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">values</span><span class="p">:</span>
                <span class="n">mean_val</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">values</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
                <span class="c1"># Convert &quot;/reward_component/name&quot; to tensorboard key</span>
                <span class="n">tb_key</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;reward_components/</span><span class="si">{</span><span class="n">key</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;/reward_component/&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;&#39;</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="n">tb_key</span><span class="p">,</span> <span class="n">mean_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">)</span>
                <span class="n">values</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">_log_environment_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Log environment-provided metrics from log buffers.&quot;&quot;&quot;</span>
        <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">buffer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">log_buffers</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">buffer</span><span class="p">:</span>
                <span class="n">mean_val</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">buffer</span><span class="p">)</span>
                <span class="c1"># Remove leading &quot;/&quot; for tensorboard key</span>
                <span class="n">tb_key</span> <span class="o">=</span> <span class="n">key</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="k">if</span> <span class="n">key</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)</span> <span class="k">else</span> <span class="n">key</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">add_scalar</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;env/</span><span class="si">{</span><span class="n">tb_key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">mean_val</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span><span class="p">)</span>
                <span class="n">buffer</span><span class="o">.</span><span class="n">clear</span><span class="p">()</span>

<div class="viewcode-block" id="OnPolicyRunner.save_checkpoint">
<a class="viewcode-back" href="../../../API/apexrl.agent.on_policy_runner.html#apexrl.agent.OnPolicyRunner.save_checkpoint">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">save_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Save training checkpoint.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="n">path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;  Saved: </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="OnPolicyRunner.load_checkpoint">
<a class="viewcode-back" href="../../../API/apexrl.agent.on_policy_runner.html#apexrl.agent.OnPolicyRunner.load_checkpoint">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">load_checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filename</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Load training checkpoint.&quot;&quot;&quot;</span>
        <span class="n">path</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">filename</span>
            <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isabs</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
            <span class="k">else</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span> <span class="ow">or</span> <span class="s2">&quot;.&quot;</span><span class="p">,</span> <span class="n">filename</span><span class="p">)</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">iteration</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> <span class="s2">&quot;iteration&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_timesteps</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="p">,</span> <span class="s2">&quot;total_timesteps&quot;</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded checkpoint: </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span></div>


<div class="viewcode-block" id="OnPolicyRunner.eval">
<a class="viewcode-back" href="../../../API/apexrl.agent.on_policy_runner.html#apexrl.agent.OnPolicyRunner.eval">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">eval</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_episodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">float</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Evaluate the agent.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">agent</span><span class="o">.</span><span class="n">eval</span><span class="p">(</span><span class="n">num_episodes</span><span class="p">)</span></div>


<div class="viewcode-block" id="OnPolicyRunner.close">
<a class="viewcode-back" href="../../../API/apexrl.agent.on_policy_runner.html#apexrl.agent.OnPolicyRunner.close">[docs]</a>
    <span class="k">def</span><span class="w"> </span><span class="nf">close</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Close runner and release resources.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">writer</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="p">,</span> <span class="s2">&quot;close&quot;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">env</span><span class="o">.</span><span class="n">close</span><span class="p">()</span></div>
</div>

</pre></div>

          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../../../index.html">ApexRL</a></h1>









<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../../../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../../../index.html">Documentation overview</a><ul>
  <li><a href="../../index.html">Module code</a><ul>
  <li><a href="../../apexrl.html">apexrl</a><ul>
  </ul></li>
  </ul></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &#169;2026, Atticlmr.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 9.0.4</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
    </div>

    

    
  </body>
</html>