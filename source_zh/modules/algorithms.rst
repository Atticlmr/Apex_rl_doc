ç®—æ³•
====

ApexRL æä¾›æœ€å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•å®ç°ã€‚

å¯ç”¨ç®—æ³•
--------

.. list-table::
   :header-rows: 1

   * - ç®—æ³•
     - ç±»å‹
     - çŠ¶æ€
     - æè¿°
   * - PPO
     - åŒç­–ç•¥
     - âœ… å¯ç”¨
     - è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–
   * - DQN
     - å¼‚ç­–ç•¥
     - ğŸš§ è®¡åˆ’ä¸­
     - æ·±åº¦ Q ç½‘ç»œ
   * - SAC
     - å¼‚ç­–ç•¥
     - ğŸš§ è®¡åˆ’ä¸­
     - è½¯ Actor-Critic

PPOï¼ˆè¿‘ç«¯ç­–ç•¥ä¼˜åŒ–ï¼‰
-------------------

PPO æ˜¯ ApexRL å½“å‰å¯ç”¨çš„ä¸»è¦ç®—æ³•ã€‚å®ƒæ˜¯ä¸€ç§ä»¥ç¨³å®šæ€§å’Œæ˜“ç”¨æ€§è‘—ç§°çš„åŒç­–ç•¥ç®—æ³•ã€‚

å…³é”®ç‰¹æ€§
~~~~~~~~

- ç”¨äºç¨³å®šæ›´æ–°çš„è£å‰ªæ›¿ä»£ç›®æ ‡
- å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼ˆGAEï¼‰
- æ”¯æŒè¿ç»­å’Œç¦»æ•£åŠ¨ä½œ
- éå¯¹ç§° Actor-Criticï¼ˆCritic æœ‰ç‰¹æƒä¿¡æ¯ï¼‰
- ç­–ç•¥/ä»·å€¼åˆ†ç¦»æˆ–è”åˆä¼˜åŒ–å™¨

åŸºæœ¬ç”¨æ³•
~~~~~~~~

.. code-block:: python

   from apexrl.algorithms.ppo import PPO, PPOConfig
   from apexrl.envs.vecenv import DummyVecEnv
   from apexrl.models.mlp import MLPActor, MLPCritic

   # åˆ›å»ºç¯å¢ƒ
   env = DummyVecEnv(num_envs=4096, num_obs=48, num_actions=12)

   # é…ç½® PPO
   cfg = PPOConfig(
       num_steps=24,
       num_epochs=5,
       learning_rate=3e-4,
       gamma=0.99,
       gae_lambda=0.95,
       clip_range=0.2,
   )

   # åˆ›å»ºæ™ºèƒ½ä½“
   agent = PPO(
       env=env,
       cfg=cfg,
       actor_class=MLPActor,
       critic_class=MLPCritic,
   )

   # è®­ç»ƒ
   agent.learn(total_timesteps=10_000_000)

é…ç½®
~~~~

.. autoclass:: apexrl.algorithms.ppo.config.PPOConfig
   :members:
   :undoc-members:

API å‚è€ƒ
~~~~~~~~

.. autoclass:: apexrl.algorithms.ppo.ppo.PPO
   :members:
   :undoc-members:
   :show-inheritance:

ç®—æ³•è¯¦æƒ…
--------

PPO-Clip ç›®æ ‡
~~~~~~~~~~~~~

PPO-Clip ç›®æ ‡å‡½æ•°ï¼š

.. math::

   L^{CLIP}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) \hat{A}_t \right) \right]

å…¶ä¸­ï¼š

- :math:`r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}` æ˜¯æ¦‚ç‡æ¯”ç‡
- :math:`\hat{A}_t` æ˜¯ä¼°è®¡ä¼˜åŠ¿
- :math:`\epsilon` æ˜¯è£å‰ªèŒƒå›´ï¼ˆé€šå¸¸ä¸º 0.2ï¼‰

æ€»æŸå¤±å‡½æ•°
~~~~~~~~~~

.. math::

   L^{TOTAL}(\theta) = L^{CLIP}(\theta) - c_1 L^{VF}(\theta) + c_2 S[\pi_\theta](s_t)

å…¶ä¸­ï¼š

- :math:`L^{VF}` æ˜¯ä»·å€¼å‡½æ•°æŸå¤±ï¼ˆMSEï¼‰
- :math:`S` æ˜¯ç†µå¥–åŠ±
- :math:`c_1`, :math:`c_2` æ˜¯ç³»æ•°

è¶…å‚æ•°è°ƒä¼˜
----------

ä¸€èˆ¬å‡†åˆ™
~~~~~~~~

.. list-table:: PPO è¶…å‚æ•°
   :header-rows: 1

   * - å‚æ•°
     - å…¸å‹èŒƒå›´
     - æè¿°
   * - ``num_steps``
     - 2048-8192
     - æ¯æ¬¡æ›´æ–°æ¯ç¯å¢ƒçš„æ­¥æ•°
   * - ``num_epochs``
     - 3-10
     - æ¯æ‰¹æ•°æ®ä¼˜åŒ–è½®æ•°
   * - ``learning_rate``
     - 1e-5 åˆ° 1e-3
     - ä¼˜åŒ–æ­¥é•¿
   * - ``gamma``
     - 0.99-0.999
     - æŠ˜æ‰£å› å­
   * - ``gae_lambda``
     - 0.9-0.99
     - GAE lambda å‚æ•°
   * - ``clip_range``
     - 0.1-0.3
     - è£å‰ªå‚æ•°
   * - ``ent_coef``
     - 0.0-0.01
     - ç†µç³»æ•°
   * - ``vf_coef``
     - 0.25-1.0
     - ä»·å€¼å‡½æ•°æŸå¤±ç³»æ•°

ç‰¹å®šç¯å¢ƒæ¨è
~~~~~~~~~~~~

**Isaac Gymï¼ˆè¶³å¼æœºå™¨äººï¼‰ï¼š**

.. code-block:: python

   cfg = PPOConfig(
       num_steps=24,
       num_epochs=5,
       learning_rate=1e-3,
       gamma=0.99,
       gae_lambda=0.95,
       clip_range=0.2,
       ent_coef=0.0,
       batch_size=98304,
       minibatch_size=32768,
   )

**Gymnasiumï¼ˆAtariï¼‰ï¼š**

.. code-block:: python

   cfg = PPOConfig(
       num_steps=128,
       num_epochs=4,
       learning_rate=2.5e-4,
       gamma=0.99,
       gae_lambda=0.95,
       clip_range=0.1,
       ent_coef=0.01,
   )

**Gymnasiumï¼ˆMujocoï¼‰ï¼š**

.. code-block:: python

   cfg = PPOConfig(
       num_steps=2048,
       num_epochs=10,
       learning_rate=3e-4,
       gamma=0.99,
       gae_lambda=0.95,
       clip_range=0.2,
       ent_coef=0.0,
   )

é«˜çº§ç‰¹æ€§
--------

å­¦ä¹ ç‡è°ƒåº¦
~~~~~~~~~~

ApexRL æ”¯æŒå¤šç§å­¦ä¹ ç‡è°ƒåº¦ï¼š

.. code-block:: python

   cfg = PPOConfig(
       learning_rate_schedule="adaptive",  # æˆ– "linear", "constant"
       max_learning_rate=1e-3,
       min_learning_rate=1e-5,
   )

- **constant**ï¼šå›ºå®šå­¦ä¹ ç‡
- **linear**ï¼šä»åˆå§‹å€¼çº¿æ€§è¡°å‡åˆ° 0
- **adaptive**ï¼šè‡ªå®šä¹‰è¡°å‡è°ƒåº¦

ä»·å€¼å‡½æ•°è£å‰ª
~~~~~~~~~~~~

å¯ç”¨ä»·å€¼å‡½æ•°è£å‰ªä»¥è·å¾—æ›´ç¨³å®šçš„è®­ç»ƒï¼š

.. code-block:: python

   cfg = PPOConfig(
       clip_range_vf=0.2,  # None è¡¨ç¤ºç¦ç”¨
   )

æ—©åœ
~~~~

å½“ KL æ•£åº¦è¶…è¿‡é˜ˆå€¼æ—¶åœæ­¢æ›´æ–°ï¼š

.. code-block:: python

   cfg = PPOConfig(
       target_kl=0.015,  # None è¡¨ç¤ºç¦ç”¨
   )

åˆ†ç¦»ä¼˜åŒ–å™¨
~~~~~~~~~~

ä¸ºç­–ç•¥å’Œä»·å€¼ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ï¼š

.. code-block:: python

   cfg = PPOConfig(
       use_policy_optimizer=True,
       policy_learning_rate=1e-4,
       value_learning_rate=3e-4,
   )

å¦è¯·å‚é˜…
--------

- :doc:`../tutorials/first_agent` - åŸºç¡€ä½¿ç”¨æ•™ç¨‹
- :doc:`../tutorials/custom_network` - è‡ªå®šä¹‰ç½‘ç»œæ¶æ„
- :doc:`../api/apexrl.algorithms.ppo` - å®Œæ•´ API å‚è€ƒ
